
\documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,scrextend}
\usepackage{fancyhdr}
\pagestyle{fancy}

\newcommand{\cont}{\subseteq}
\usepackage{tikz}
\usepackage{circuitikz}

\usepackage{pgfplots}
\usepackage{amsmath}
\usepackage[mathscr]{euscript}
\let\euscr\mathscr \let\mathscr\relax% just so we can load this and rsfs
\usepackage[scr]{rsfso}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{multicol}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[colorlinks=true, pdfstartview=FitV, linkcolor=blue,
citecolor=blue, urlcolor=blue]{hyperref}

\DeclareMathOperator{\arcsec}{arcsec}
\DeclareMathOperator{\arccot}{arccot}
\DeclareMathOperator{\arccsc}{arccsc}
\newcommand{\ddx}{\frac{d}{dx}}
\algrenewcommand\algorithmicrequire{\textbf{Input:}}
\algrenewcommand\algorithmicensure{\textbf{Output:}}
\newcommand{\dfdx}{\frac{df}{dx}}
\newcommand{\ddxp}[1]{\frac{d}{dx}\left( #1 \right)}
\newcommand{\dydx}{\frac{dy}{dx}}
\let\ds\displaystyle
\newcommand{\intx}[1]{\int #1 \, dx}
\newcommand{\intt}[1]{\int #1 \, dt}
\newcommand{\defint}[3]{\int_{#1}^{#2} #3 \, dx}
\newcommand{\imp}{\Rightarrow}
\newcommand{\un}{\cup}
\newcommand{\inter}{\cap}
\newcommand{\ps}{\mathscr{P}}
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newtheorem*{sol}{Solution}
\newtheorem*{claim}{Claim}
\newtheorem{problem}{Problem}
\setlength{\parindent}{0pt}
\begin{document}
 
% EVERYTHING ABOVE THIS LINE IS JUST PREABLE, NO NEED TO MESS WITH IT.__________________________________________________________________________________________
%

\lhead{SMGT 490}
\chead{Lou Zhou}
\rhead{\today}
 
% \maketitle
\begin{center}
\textbf{\Large{Previous Works}}
\end{center}
Optimal decision-making through a win-probability lens is a long been studied problem in traditional sports, where analysts seek to evaluate in-game decisions by their impact on winning probability than on immediate outcomes. In Baseball, \href{https://www.degruyterbrill.com/document/doi/10.1515/jqas-2017-0092/html?lang=en}{Hirotsu and Bickel (2019)} use Markov Decision Processes (MDP) to model whether a sacrifice bunt would be optimal by comparing the total win-probabilities of bunting and swinging away across a variety of different game states. 
\bigbreak
In American Football, the decision to ``go for it'' on fourth down is a frequently studied problem from a win-probability maximization lens \href{https://www.tandfonline.com/doi/full/10.1080/00031305.2025.2475801}{(Brill, Yurko, and Wyner, 2025)}. Similar approaches can be found in soccer (\href{https://www.sloansportsconference.com/research-papers/beyond-action-valuation-a-deep-reinforcement-learning-framework-for-optimizing-player-decisions-in-soccer}{Rahimian, et al. (2022)}), Tennis (\href{https://web.stanford.edu/class/aa228/reports/2018/final84.pdf}{Rousoglou, 2018}), and Basketball (\href{https://journals.sagepub.com/doi/10.3233/JSA-180231}{McFarlane (2019)}). 
\bigbreak
Across these traditional sports settings, a unifying theme is that optimal decision-making often involves sacrificing short-term success in order to improve long-term outcomes. Whether through advancing a runner at the cost of an out or declining a field goal attempt in favor of a fourth-down conversion, these decisions are best evaluated through their impact on the probability of winning the game rather than immediate results. 
\bigbreak
With the growth of the avaliability of data in esports with match replay parsers like awpy (\href{https://arxiv.org/pdf/2011.01324}{Xenopoulus (2020)}) increasing the ease-of-access to both event and tracking data, similar win-probability approaches have begun to emerge. Numerous win-probability approaches have been developed across many esports. \href{https://arxiv.org/pdf/1701.03162}{Yang et. al (2016)} build a logistic regression and deep learning approach to predict the winner of Defense of the Ancients 2 (Dota 2) games. In a similar game, \href{https://cdr.lib.unc.edu/concern/masters_papers/8s45qd54c}{Tian (2019)} uses logistic regression and tree-based methods to predict the winner of League of Legends matches. 
\bigbreak

Using these win-probability approaches, previous works have extended these models to evaluate player actions. For example, \href{https://sal.aalto.fi/publications/pdf-files//theses/mas/tjal24a_public.pdf}{Jalovaara (2024)} uses win-probability models in League of Legends to evaluate the effectiveness of purchasable items in League of Legends. 
\bigbreak 
Similarly, \href{https://www.sloansportsconference.com/research-papers/evaluating-player-actions-in-professional-counter-strike-using-temporal-heterogeneous-graph-neural-networks}{Szmide and Toka (2025)} build a Temporal Heterogeneous Graph Neural Network to predict round win-probabilities based on tracking data. Using this approach, we can evaluate player decisions from a round win-probability lens. However, given the sequential nature of rounds, where saved weapons and utility items are carried over to the next round, the work does not account for situations where the optimal decision to maximize match win probability might differ from the best decision to win the current round, with saving powerful weapons but giving up on the round as an example. 
\bigbreak
More generally, round-based win-probability-based evaluation methods do not account for their impact on future rounds, severely underestimating the actions of saving. In games like Counter-Strike, where round win-probability is not independent, actions that reduce the probability of winning the current round could still increase the probability of winning future rounds or the match as a whole by preserving weapons or utility weapons. This difference motivates the usage of win-probability models, especially in regards to evaluating saving.
\bigbreak
Without a win-probability approach, we cannot distinguish between decisions that are suboptimal and decisions that appear ineffective due to unfavorable underlying states. This work seeks to extend community analysis by building formal mathematical models that explicitly evaluate alternative actions under comparable conditions.

\bigbreak
At the team level, win-probability models have also been used to evaluate higher-level strategic decisions. \href{https://arxiv.org/pdf/2106.08888}{Petri et. al (2021)} uses a bandit-based reinforcement learning approach to find the map veto decision to maximize the probability of winning a match. In a similar vein, \href{https://arxiv.org/pdf/2106.08888}{Xenopoulos et. al (2021)} uses gradient boosting approaches to predict game win probabilities and evaluate team-level economic decisions that influence match outcomes.
\bigbreak
In a more informal setting, \href{https://www.hltv.org/news/35622/saving-how-bad-is-it}{Harry Richards (2023)} on Half-Life TV, gives a high-level overview of saving in Counter-Strike by looking at teams and players who tend to save the most and the correlation of saving with win probability in following rounds. However, this analysis is primarily descriptive and retrospective in nature, focusing on observed outcomes and basic statistics. 
\bigbreak
However, one of the central challenges in evaluating in-game decisions using observed data is selection bias, which the actions taken influence which observations are observed. In American football, \href{https://www.sloansportsconference.com/research-papers/correcting-for-preferential-bias-in-nfl-fourth-down-decision-making}{Daly-Grafstein (2023)} notes that better teams are more likely to attempt to convert fourth-downs while worse teams will be put in situations where it is imperative that they go for it. With this difference, biased probability estimates can arise. As a result, the work uses a generalized Heckman selection model for fourth-down plays, treating the decision to go-for-it on fourth down as a missing data problem. 
\bigbreak
More broadly, selection bias in sports analytics arises when the probability of observing an outcome depends on the action taken. In decision-making contexts, observed outcomes may not be representative of the outcomes that would occur if that action were taken, leading to potential bias in naive win-probability estimates derived purely from observed data.
\bigbreak
Similarly, with saving in Counter-Strike, building saving models has the induced challenge where weaker teams are much more likely to be saving since they are more likely to lose rounds in general. While, on the other hand, stronger teams likely save less since they are less likely to have rounds where they might need to save. This difference can generate bias in modeling saving as we have this clear mis-match in the level of teams that are more likely be in situations of saving. As in fourth-down decision-making, this induces a mismatch between observed outcomes and the true value of other actions, complicating attempts to assess whether saving is optimal in a given state.
\bigbreak
Taken together, existing work has applied win-probability approaches for outcome prediction and action evaluation in esports, but leaves open the question of how to evaluate mid-round, sequential decisions. In particular, there is limited prior work that assesses saving decisions in Counter-Strike through a win-probability framework that models this type of mid-round decision. The present work seeks to address this gap by framing saving as a win-probability optimization problem accounting for both mid-round state and long-term winning probability. In doing so, we are able to build a principled framework for assessing when sacrificing the current round may be optimal in maximizing the probability of winning the match.

% THE DOCUMENT IS ESSENTIALLY DONE AT THIS POINT, NO NEED TO EDIT ANYTHING BELOW THIS______________________________________________________________________________________________
 
\end{document}